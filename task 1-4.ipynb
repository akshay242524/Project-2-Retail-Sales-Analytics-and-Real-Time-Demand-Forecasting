{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9T7EmiivXI6"
   },
   "source": [
    "# **Project 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maZtWtQ05Z2v"
   },
   "source": [
    "## Task 1: Data Loading, Cleansing, and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GIoMSP9vcnX",
    "outputId": "20d7ef6a-6c1f-44ac-fa0c-371e3b58f6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Preview - Reviews DataFrame:\n",
      "+-------------------------------------------------------------------------------------------+---------+\n",
      "|Review                                                                                     |Sentiment|\n",
      "+-------------------------------------------------------------------------------------------+---------+\n",
      "|This product exceeded my expectations! It's high-quality and performs exceptionally well.  |Positive |\n",
      "|The product was decent. It worked fine, but it wasn't anything special.                    |Neutral  |\n",
      "|I had a terrible experience with this company. The customer service was rude and unhelpful.|Negative |\n",
      "|It's an okay product. Nothing to write home about.                                         |Neutral  |\n",
      "|Disappointed with the product. It didn't meet my expectations.                             |Negative |\n",
      "+-------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Initial Data Preview - Online Retail DataFrame:\n",
      "+---------+---------+-----------------------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate     |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+----------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |01-12-2010 08:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |01-12-2010 08:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |01-12-2010 08:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |01-12-2010 08:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |01-12-2010 08:26|3.39     |17850     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema for Reviews DataFrame:\n",
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n",
      "Schema for Online Retail DataFrame:\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "Missing values in Reviews DataFrame:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "|     0|        0|\n",
      "+------+---------+\n",
      "\n",
      "Missing values in Online Retail DataFrame:\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|        142|       0|          0|        0|     17881|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n",
      "Data Preview after Cleansing - Reviews DataFrame:\n",
      "+--------------------------------------------------+---------+\n",
      "|Review                                            |Sentiment|\n",
      "+--------------------------------------------------+---------+\n",
      "|Great product! Exceeded my expectations.          |Positive |\n",
      "|Not bad, but not great either. Just average.      |Neutral  |\n",
      "|The product quality was mediocre. Expected better.|Neutral  |\n",
      "|Incredible product! Absolutely love it.           |Positive |\n",
      "|Meh. The product was neither good nor bad.        |Neutral  |\n",
      "+--------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Data Preview after Cleansing - Online Retail DataFrame:\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |2010-12-01 08:26:00|2.75     |17850     |United Kingdom|\n",
      "|536406   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|8       |2010-12-01 11:33:00|2.55     |17850     |United Kingdom|\n",
      "|536423   |22619    |SET OF 6 SOLDIER SKITTLES         |4       |2010-12-01 12:08:00|3.75     |18085     |United Kingdom|\n",
      "|536446   |22294    |HEART FILIGREE DOVE  SMALL        |48      |2010-12-01 12:15:00|1.25     |15983     |United Kingdom|\n",
      "|536520   |22241    |GARLAND WOODEN HAPPY EASTER       |2       |2010-12-01 12:43:00|1.25     |14729     |United Kingdom|\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Post-cleansing number of rows in Reviews DataFrame: 131\n",
      "Post-cleansing number of rows in Online Retail DataFrame: 31042\n",
      "Summary Statistics for Reviews DataFrame:\n",
      "+-------+--------------------+---------+\n",
      "|summary|              Review|Sentiment|\n",
      "+-------+--------------------+---------+\n",
      "|  count|                 131|      131|\n",
      "|   mean|                NULL|     NULL|\n",
      "| stddev|                NULL|     NULL|\n",
      "|    min|Absolutely atroci...| Negative|\n",
      "|    max|Would not recomme...| Positive|\n",
      "+-------+--------------------+---------+\n",
      "\n",
      "Summary Statistics for Online Retail DataFrame:\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|            31042|             31042|               31042|             31042|             31042|             31042|         31042|\n",
      "|   mean|538355.8431775267|28764.722584597643|                NULL|11.571000579859545|3.2392980478057316|15412.855969331873|          NULL|\n",
      "| stddev|1223.745446554134|18661.784642859104|                NULL| 69.37261221287764| 8.624136936996798|1755.5902947940654|          NULL|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|             -9360|               0.0|             12347|     Australia|\n",
      "|    max|          C540503|              POST|ZINC WILLIE WINKI...|              2880|            1126.0|             18283|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n",
      "Distinct Sentiments in Reviews DataFrame:\n",
      "+---------+\n",
      "|Sentiment|\n",
      "+---------+\n",
      "| Positive|\n",
      "|  Neutral|\n",
      "| Negative|\n",
      "+---------+\n",
      "\n",
      "Distinct Countries in Online Retail DataFrame:\n",
      "+---------------+\n",
      "|        Country|\n",
      "+---------------+\n",
      "|         Sweden|\n",
      "|        Germany|\n",
      "|         France|\n",
      "|        Belgium|\n",
      "|        Finland|\n",
      "|          Italy|\n",
      "|           EIRE|\n",
      "|      Lithuania|\n",
      "|         Norway|\n",
      "|          Spain|\n",
      "|        Denmark|\n",
      "|        Iceland|\n",
      "|         Israel|\n",
      "|Channel Islands|\n",
      "|         Cyprus|\n",
      "|    Switzerland|\n",
      "|          Japan|\n",
      "|         Poland|\n",
      "|       Portugal|\n",
      "|      Australia|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, count, when, regexp_extract\n",
    "\n",
    "# Step 2: Initialize PySpark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail and Sentiment Analytics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 3: Load Datasets\n",
    "sentiment_df = spark.read.csv(\"reviews.csv\", header=True, inferSchema=True)\n",
    "retail_df = spark.read.csv(\"Copy of Online Retail.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 4: Preview the Data\n",
    "print(\"Initial Data Preview - Reviews DataFrame:\")\n",
    "sentiment_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Initial Data Preview - Online Retail DataFrame:\")\n",
    "retail_df.show(5, truncate=False)\n",
    "\n",
    "# Step 5: Confirm Schema\n",
    "print(\"Schema for Reviews DataFrame:\")\n",
    "sentiment_df.printSchema()\n",
    "\n",
    "print(\"Schema for Online Retail DataFrame:\")\n",
    "retail_df.printSchema()\n",
    "\n",
    "# Step 6: Handle Missing Values\n",
    "print(\"Missing values in Reviews DataFrame:\")\n",
    "sentiment_df.select([count(when(col(c).isNull(), c)).alias(c) for c in sentiment_df.columns]).show()\n",
    "\n",
    "print(\"Missing values in Online Retail DataFrame:\")\n",
    "retail_df.select([count(when(col(c).isNull(), c)).alias(c) for c in retail_df.columns]).show()\n",
    "\n",
    "# Drop rows with missing values\n",
    "sentiment_df = sentiment_df.dropna(how='any')\n",
    "retail_df = retail_df.dropna(how='any')\n",
    "\n",
    "# Step 7: Remove Duplicates\n",
    "sentiment_df = sentiment_df.dropDuplicates()\n",
    "retail_df = retail_df.dropDuplicates()\n",
    "\n",
    "# Step 8: Validate and Convert Dates in Online Retail DataFrame\n",
    "# Add a column to check if InvoiceDate is in the expected format\n",
    "retail_df = retail_df.withColumn(\n",
    "    \"ValidDate\", regexp_extract(col(\"InvoiceDate\"), r\"\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}\", 0)\n",
    ")\n",
    "\n",
    "# Filter out rows with invalid InvoiceDate\n",
    "invalid_dates = retail_df.filter(col(\"ValidDate\") == \"\").select(\"InvoiceDate\").distinct()\n",
    "if invalid_dates.count() > 0:\n",
    "    print(\"Rows with invalid InvoiceDate format:\")\n",
    "    invalid_dates.show()\n",
    "\n",
    "retail_df = retail_df.filter(col(\"ValidDate\") != \"\").drop(\"ValidDate\")\n",
    "\n",
    "# Convert InvoiceDate to Timestamp\n",
    "retail_df = retail_df.withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"dd-MM-yyyy HH:mm\"))\n",
    "\n",
    "# Step 9: Post-Cleansing Preview\n",
    "print(\"Data Preview after Cleansing - Reviews DataFrame:\")\n",
    "sentiment_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Data Preview after Cleansing - Online Retail DataFrame:\")\n",
    "retail_df.show(5, truncate=False)\n",
    "\n",
    "# Step 10: Post-Cleansing Row Count\n",
    "print(\"Post-cleansing number of rows in Reviews DataFrame:\", sentiment_df.count())\n",
    "print(\"Post-cleansing number of rows in Online Retail DataFrame:\", retail_df.count())\n",
    "\n",
    "# Step 11: Summary Statistics for Validation\n",
    "print(\"Summary Statistics for Reviews DataFrame:\")\n",
    "sentiment_df.describe().show()\n",
    "\n",
    "print(\"Summary Statistics for Online Retail DataFrame:\")\n",
    "retail_df.describe().show()\n",
    "\n",
    "# Step 12: Additional Checks\n",
    "# Check distinct values in the Sentiment column of Reviews DataFrame\n",
    "print(\"Distinct Sentiments in Reviews DataFrame:\")\n",
    "sentiment_df.select(\"Sentiment\").distinct().show()\n",
    "\n",
    "# Check distinct countries in the Online Retail DataFrame\n",
    "print(\"Distinct Countries in Online Retail DataFrame:\")\n",
    "retail_df.select(\"Country\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1XregceHFoX"
   },
   "source": [
    "## Task 2: Sales Data Aggregation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpwHFDaVExO6",
    "outputId": "22037894-d1d7-4f1e-af3e-feb59eeebabf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales Per Product Per Month:\n",
      "+---------+----+-----+------------------+\n",
      "|StockCode|Year|Month|TotalSales        |\n",
      "+---------+----+-----+------------------+\n",
      "|22423    |2010|12   |16608.900000000005|\n",
      "|85123A   |2010|12   |8245.649999999994 |\n",
      "|21623    |2010|12   |6938.489999999998 |\n",
      "|82484    |2010|12   |6672.330000000001 |\n",
      "|21137    |2010|12   |6248.820000000001 |\n",
      "|79321    |2010|12   |6199.34           |\n",
      "|22189    |2010|12   |5532.630000000002 |\n",
      "|22188    |2010|12   |5157.790000000001 |\n",
      "|22328    |2010|12   |4769.999999999997 |\n",
      "|22086    |2010|12   |4575.799999999996 |\n",
      "+---------+----+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Average Revenue Per Customer Segment:\n",
      "+-------------------+-----------------+\n",
      "|    CustomerSegment|       AvgRevenue|\n",
      "+-------------------+-----------------+\n",
      "|Low-Value Customers|619.9361326530609|\n",
      "|Mid-Value Customers|970.1955319148936|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "Seasonal Patterns for Top-Selling Products:\n",
      "+---------+-----+------------------+\n",
      "|StockCode|Month|MonthlySales      |\n",
      "+---------+-----+------------------+\n",
      "|22423    |12   |16608.900000000005|\n",
      "|85123A   |12   |8245.649999999994 |\n",
      "|21623    |12   |6938.489999999998 |\n",
      "|82484    |12   |6672.330000000001 |\n",
      "|21137    |12   |6248.820000000001 |\n",
      "|79321    |12   |6199.34           |\n",
      "|22189    |12   |5532.630000000002 |\n",
      "|22188    |12   |5157.790000000001 |\n",
      "|22328    |12   |4769.999999999997 |\n",
      "|22086    |12   |4575.799999999996 |\n",
      "+---------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Customer Lifetime Value:\n",
      "+----------+---------------------+\n",
      "|CustomerID|CustomerLifetimeValue|\n",
      "+----------+---------------------+\n",
      "|15727     |1516.9500000000003   |\n",
      "|17420     |130.85000000000002   |\n",
      "|16503     |477.59000000000003   |\n",
      "|16861     |-22.110000000000003  |\n",
      "|17389     |-442.5               |\n",
      "|12471     |3422.2100000000014   |\n",
      "|16916     |196.43               |\n",
      "|15100     |492.75               |\n",
      "|15738     |326.70000000000005   |\n",
      "|17809     |1251.5               |\n",
      "+----------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Product Popularity Score:\n",
      "+---------+-----------------+---------------+------------------+------------------+\n",
      "|StockCode|TotalQuantitySold|UniqueCustomers|TotalRevenue      |PopularityScore   |\n",
      "+---------+-----------------+---------------+------------------+------------------+\n",
      "|22423    |1700             |142            |19122.90000000001 |20964.90000000001 |\n",
      "|85123A   |3996             |181            |10647.400000000005|14824.400000000005|\n",
      "|79321    |2295             |52             |9616.59           |11963.59          |\n",
      "|85099B   |3257             |94             |5635.049999999998 |8986.05           |\n",
      "|21137    |2015             |20             |6852.09           |8887.09           |\n",
      "|82484    |1393             |45             |7026.930000000001 |8464.93           |\n",
      "|21623    |1063             |27             |6978.289999999999 |8068.289999999999 |\n",
      "|22189    |2299             |29             |5714.33           |8042.33           |\n",
      "|22834    |3534             |120            |4223.65           |7877.65           |\n",
      "|22188    |2156             |21             |5169.639999999999 |7346.639999999999 |\n",
      "+---------+-----------------+---------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Seasonal Trends:\n",
      "+---------+-----+--------------+------+\n",
      "|StockCode|Month|MonthlyRevenue|Trend |\n",
      "+---------+-----+--------------+------+\n",
      "|20711    |1    |553.5         |Winter|\n",
      "|21164    |1    |14.75         |Winter|\n",
      "|84978    |12   |323.89        |Winter|\n",
      "|37446    |1    |100.0         |Winter|\n",
      "|15060B   |12   |172.5         |Winter|\n",
      "|22449    |12   |190.95        |Winter|\n",
      "|84978    |1    |83.16         |Winter|\n",
      "|20998    |12   |11.8          |Winter|\n",
      "|21122    |12   |555.28        |Winter|\n",
      "|79403    |12   |0.85          |Winter|\n",
      "+---------+-----+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Register DataFrame as SQL Temporary View\n",
    "retail_df.createOrReplaceTempView(\"online_retail\")\n",
    "\n",
    "# Step 2: Total Sales Per Product Per Month\n",
    "sales_per_product_month = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        StockCode,\n",
    "        YEAR(InvoiceDate) AS Year,\n",
    "        MONTH(InvoiceDate) AS Month,\n",
    "        SUM(Quantity * UnitPrice) AS TotalSales\n",
    "    FROM online_retail\n",
    "    GROUP BY StockCode, Year, Month\n",
    "    ORDER BY Year, Month, TotalSales DESC\n",
    "\"\"\")\n",
    "print(\"Total Sales Per Product Per Month:\")\n",
    "sales_per_product_month.show(10, truncate=False)\n",
    "\n",
    "# Step 3: Average Revenue Per Customer Segment\n",
    "# Assuming `CustomerID` segments can be identified or grouped into ranges\n",
    "avg_revenue_per_segment = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerID,\n",
    "        SUM(Quantity * UnitPrice) AS TotalRevenue,\n",
    "        CASE\n",
    "            WHEN CustomerID < 18000 THEN 'Low-Value Customers'\n",
    "            WHEN CustomerID BETWEEN 18000 AND 19000 THEN 'Mid-Value Customers'\n",
    "            ELSE 'High-Value Customers'\n",
    "        END AS CustomerSegment\n",
    "    FROM online_retail\n",
    "    GROUP BY CustomerID\n",
    "\"\"\")\n",
    "avg_revenue_per_segment = avg_revenue_per_segment.groupBy(\"CustomerSegment\").agg(\n",
    "    F.avg(\"TotalRevenue\").alias(\"AvgRevenue\")\n",
    ")\n",
    "print(\"Average Revenue Per Customer Segment:\")\n",
    "avg_revenue_per_segment.show()\n",
    "\n",
    "# Step 4: Seasonal Patterns for Top-Selling Products\n",
    "seasonal_patterns = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        StockCode,\n",
    "        MONTH(InvoiceDate) AS Month,\n",
    "        SUM(Quantity * UnitPrice) AS MonthlySales\n",
    "    FROM online_retail\n",
    "    GROUP BY StockCode, Month\n",
    "    ORDER BY MonthlySales DESC\n",
    "\"\"\")\n",
    "print(\"Seasonal Patterns for Top-Selling Products:\")\n",
    "seasonal_patterns.show(10, truncate=False)\n",
    "\n",
    "# Step 5: Feature Engineering\n",
    "\n",
    "# Derive Customer Lifetime Value (CLV)\n",
    "customer_lifetime_value = retail_df.groupBy(\"CustomerID\").agg(\n",
    "    F.sum(F.col(\"Quantity\") * F.col(\"UnitPrice\")).alias(\"CustomerLifetimeValue\")\n",
    ")\n",
    "print(\"Customer Lifetime Value:\")\n",
    "customer_lifetime_value.show(10, truncate=False)\n",
    "\n",
    "# Derive Product Popularity Score\n",
    "# Popularity = Total Quantity Sold + Number of Unique Customers + Total Revenue\n",
    "product_popularity = retail_df.groupBy(\"StockCode\").agg(\n",
    "    F.sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "    F.countDistinct(\"CustomerID\").alias(\"UniqueCustomers\"),\n",
    "    F.sum(F.col(\"Quantity\") * F.col(\"UnitPrice\")).alias(\"TotalRevenue\")\n",
    ")\n",
    "product_popularity = product_popularity.withColumn(\n",
    "    \"PopularityScore\",\n",
    "    F.col(\"TotalQuantitySold\") + F.col(\"UniqueCustomers\") + F.col(\"TotalRevenue\")\n",
    ")\n",
    "print(\"Product Popularity Score:\")\n",
    "product_popularity.orderBy(F.desc(\"PopularityScore\")).show(10, truncate=False)\n",
    "\n",
    "# Derive Seasonal Trends\n",
    "seasonal_trends = retail_df.withColumn(\"Month\", F.month(\"InvoiceDate\"))\n",
    "seasonal_trends = seasonal_trends.groupBy(\"StockCode\", \"Month\").agg(\n",
    "    F.sum(F.col(\"Quantity\") * F.col(\"UnitPrice\")).alias(\"MonthlyRevenue\")\n",
    ")\n",
    "seasonal_trends = seasonal_trends.withColumn(\n",
    "    \"Trend\",\n",
    "    F.when(F.col(\"Month\").isin([12, 1, 2]), \"Winter\")\n",
    "    .when(F.col(\"Month\").isin([3, 4, 5]), \"Spring\")\n",
    "    .when(F.col(\"Month\").isin([6, 7, 8]), \"Summer\")\n",
    "    .when(F.col(\"Month\").isin([9, 10, 11]), \"Autumn\")\n",
    ")\n",
    "print(\"Seasonal Trends:\")\n",
    "seasonal_trends.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66yxG5CcIzj5"
   },
   "source": [
    "\n",
    "## Task 3: Demand Forecasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzRyF8fnI0CN",
    "outputId": "589f92ca-e5db-4b18-e23b-8ab4a23270f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 109.97027900797575\n",
      "Mean Absolute Error (MAE): 86.26772043009171\n",
      "Sample Predictions:\n",
      "+------------------------------------------+----------+------------------+\n",
      "|features                                  |DailySales|prediction        |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|[0.0,192.0,562.0,63.0,24.0,33.0,24.0]     |8         |90.70934065160615 |\n",
      "|[1.0,13.0,48.0,16.0,97.0,37.0,149.0]      |2         |66.97378520138594 |\n",
      "|[2.0,1.0,2.0,2.0,1.0,2.0,10.0]            |1         |64.28218979835289 |\n",
      "|[2.0,10.0,1.0,10.0,2.0,3.0,1008.0]        |1         |62.32870598085804 |\n",
      "|[10.0,1.0,10.0,2.0,3.0,1008.0,2.0]        |2         |160.86110754405468|\n",
      "|[14.0,6.0,6.0,2.0,10.0,27.0,18.0]         |24        |66.06005655063524 |\n",
      "|[18.0,26.0,3.0,30.0,4.0,18.0,20.0]        |27        |65.40887860570464 |\n",
      "|[24.0,328.0,69.0,80.0,58.0,1.0,14.0]      |24        |95.69075514482255 |\n",
      "|[42.0,151.0,188.0,149.0,321.0,136.0,198.0]|303       |78.3688137996204  |\n",
      "|[47.0,23.0,36.0,59.0,42.0,-468.0,790.0]   |66        |11.217970610142643|\n",
      "+------------------------------------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Prepare the Data for Time Series Forecasting\n",
    "# Filter popular products (example: Top 5 products by total sales)\n",
    "popular_products = retail_df.groupBy(\"StockCode\").agg(\n",
    "    F.sum(F.col(\"Quantity\") * F.col(\"UnitPrice\")).alias(\"TotalRevenue\")\n",
    ").orderBy(F.desc(\"TotalRevenue\")).limit(5).select(\"StockCode\")\n",
    "\n",
    "popular_data = retail_df.join(popular_products, on=\"StockCode\", how=\"inner\")\n",
    "\n",
    "# Aggregate data by StockCode and Date (remove time granularity)\n",
    "popular_data = popular_data.withColumn(\"InvoiceDate\", F.to_date(\"InvoiceDate\"))\n",
    "daily_sales = popular_data.groupBy(\"StockCode\", \"InvoiceDate\").agg(\n",
    "    F.sum(\"Quantity\").alias(\"DailySales\")\n",
    ")\n",
    "\n",
    "# Create Lag Features for Time Series Modeling\n",
    "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"InvoiceDate\")\n",
    "for lag in range(1, 8):  # Create 7 lag features for the past week\n",
    "    daily_sales = daily_sales.withColumn(f\"lag_{lag}\", F.lag(\"DailySales\", lag).over(window_spec))\n",
    "\n",
    "# Remove rows with null values (incomplete lag features)\n",
    "daily_sales = daily_sales.dropna()\n",
    "\n",
    "# Step 2: Assemble Features and Labels\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"lag_{lag}\" for lag in range(1, 8)],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "daily_sales = assembler.transform(daily_sales).select(\"features\", \"DailySales\")\n",
    "\n",
    "# Step 3: Split Data into Training and Testing\n",
    "train_data, test_data = daily_sales.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 4: Build the Regression Model Pipeline\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"DailySales\", predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[scaler, lr])\n",
    "\n",
    "# Step 5: Hyperparameter Tuning with CrossValidator\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"DailySales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Step 6: Train the Model\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Step 7: Evaluate the Model on Test Data\n",
    "predictions = cv_model.transform(test_data)\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Step 8: Display Predictions\n",
    "print(\"Sample Predictions:\")\n",
    "predictions.select(\"features\", \"DailySales\", \"prediction\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG8wTivsLzcX"
   },
   "source": [
    "## Task 4: Sentiment Analysis on Customer Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N83YIyEeLz0Y",
    "outputId": "cfaeadc0-2153-4873-aff7-78a99e84f418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.91\n",
      "+--------------------+---------+---------------+\n",
      "|              Review|Sentiment|sentiment_score|\n",
      "+--------------------+---------+---------------+\n",
      "|this product exce...| Positive|            1.0|\n",
      "|the product was d...|  Neutral|            0.0|\n",
      "|i had a terrible ...| Negative|           -1.0|\n",
      "|its an okay produ...|  Neutral|            0.0|\n",
      "|disappointed with...| Negative|           -1.0|\n",
      "|avoid this compan...| Negative|           -1.0|\n",
      "|i had a terrible ...| Negative|           -1.0|\n",
      "|avoid this compan...| Negative|           -1.0|\n",
      "|this product exce...| Positive|            1.0|\n",
      "|this product is o...| Positive|            1.0|\n",
      "|absolutely horren...| Negative|           -1.0|\n",
      "|i had a terrible ...| Negative|           -1.0|\n",
      "|this experience w...|  Neutral|            0.0|\n",
      "|topquality produc...| Positive|            1.0|\n",
      "|im extremely diss...| Negative|           -1.0|\n",
      "|the service was s...|  Neutral|            0.0|\n",
      "|the product was d...|  Neutral|            0.0|\n",
      "|i would not recom...| Negative|           -1.0|\n",
      "|the service was s...|  Neutral|            0.0|\n",
      "|the customer serv...| Positive|            1.0|\n",
      "+--------------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------------+\n",
      "|              Review|aggregate_sentiment_score|\n",
      "+--------------------+-------------------------+\n",
      "|fantastic custome...|                      1.0|\n",
      "|absolutely horren...|                     -1.0|\n",
      "|absolutely terrib...|                     -1.0|\n",
      "|just an okay prod...|                      0.0|\n",
      "|terrible product ...|                     -1.0|\n",
      "|brilliant product...|                      1.0|\n",
      "|the quality of th...|                      1.0|\n",
      "|amazing service e...|                      1.0|\n",
      "|disappointed with...|                     -1.0|\n",
      "|just an average p...|                      0.0|\n",
      "|my experience was...|                      0.0|\n",
      "|i had a terrible ...|                    -22.0|\n",
      "|its an average pr...|                      0.0|\n",
      "|the experience wa...|                      0.0|\n",
      "|highly impressed ...|                      1.0|\n",
      "|great product exc...|                      1.0|\n",
      "|impressed with th...|                      1.0|\n",
      "|regret buying thi...|                     -1.0|\n",
      "|the service was a...|                      0.0|\n",
      "|the product was m...|                      0.0|\n",
      "+--------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, when, regexp_replace, lower\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Reviews Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the reviews dataset\n",
    "file_path = \"/content/reviews.csv\"  # Replace with your dataset path\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Preprocessing: Cleaning text\n",
    "data_cleaned = data.withColumn(\"Review\", regexp_replace(col(\"Review\"), \"[^a-zA-Z\\\\s]\", \"\")) \\\n",
    "                   .withColumn(\"Review\", lower(col(\"Review\")))\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"tokens\")\n",
    "\n",
    "# Remove stop words\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "# Generate n-grams\n",
    "ngram = NGram(n=2, inputCol=\"filtered_tokens\", outputCol=\"ngrams\")\n",
    "\n",
    "# Convert text labels to numeric (StringIndexer)\n",
    "label_indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
    "\n",
    "# Convert tokens to feature vectors\n",
    "vectorizer = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\")\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, ngram, label_indexer, vectorizer, lr])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = data_cleaned.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Set Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "# UDF to assign sentiment scores\n",
    "sentiment_to_score = udf(lambda sentiment: 1.0 if sentiment == \"Positive\" else\n",
    "                         (-1.0 if sentiment == \"Negative\" else 0.0), DoubleType())\n",
    "\n",
    "# Add sentiment_score column\n",
    "scored_data = data_cleaned.withColumn(\n",
    "    \"sentiment_score\",\n",
    "    when(col(\"Sentiment\").isNull(), 0.0).otherwise(sentiment_to_score(col(\"Sentiment\")))\n",
    ")\n",
    "\n",
    "# Debug: Verify sentiment scores\n",
    "scored_data.select(\"Review\", \"Sentiment\", \"sentiment_score\").show()\n",
    "\n",
    "# Aggregate sentiment score for each review\n",
    "aggregate_score = scored_data.groupBy(\"Review\") \\\n",
    "    .agg({\"sentiment_score\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(sentiment_score)\", \"aggregate_sentiment_score\")\n",
    "\n",
    "# Display aggregate sentiment scores\n",
    "aggregate_score.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
